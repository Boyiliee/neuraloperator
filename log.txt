nohup: ignoring input
wandb: Currently logged in as: boyili (nvr-ai-algo). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.15.1
wandb: Run data is saved locally in logs/2023-07-03_20-27-04/wandb/run-20230703_202706-638p52cm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run FNOGNO-linear
wandb: ‚≠êÔ∏è View project at https://wandb.ai/nvr-ai-algo/ahmed-body
wandb: üöÄ View run at https://wandb.ai/nvr-ai-algo/ahmed-body/runs/638p52cm
/home/boyil/programfiles/anaconda3/envs/geono/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:381: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
Warning: Open3D was built with CUDA 11.6 butPyTorch was built with CUDA 11.7. Falling back to CPU for now.Otherwise, install PyTorch with CUDA 11.6.
Namespace(batch_size=None, checkpoint=None, config='config/FNOGNOAhmed.yaml', data_path=None, device='cuda', log='log', logger_types=['wandb'], lr=None, model=None, num_epochs=None, output='output', sdf_spatial_resolution=None, seed=0)
lr: 0.001
num_epochs: 100
batch_size: 1
logger_types: ['wandb']
log_dir: logs/
eval_interval: 5
run_name: FNOGNO-linear
project_name: ahmed-body
entity: nvr-ai-algo
data_path: ahmed-body-dataset
data_module: AhmedBodyDataModule
sdf_spatial_resolution: [64, 64, 64]
train_ratio: 0.8
val_ratio: 0.1
test_ratio: 0.1
model: FNOGNO
opt_scheduler: StepLR
opt_step_size: 50
opt_gamma: 0.5
test_plot_interval: 20
device: cuda
output: output
log: log
seed: 0
Padding inputs of resolution=torch.Size([64, 64, 64]) with padding=[8, 8, 8], one-sided
Training epoch 0 took 141.80 seconds. L2 loss: 0.9463
Epoch: 0 l2: 0.9227
Epoch: 0 l2_decoded: 0.8959
Training epoch 1 took 141.09 seconds. L2 loss: 0.9480
Training epoch 2 took 141.24 seconds. L2 loss: 0.9469
Training epoch 3 took 140.95 seconds. L2 loss: 0.9455
Training epoch 4 took 141.12 seconds. L2 loss: 0.9453
Training epoch 5 took 140.99 seconds. L2 loss: 0.9454
Epoch: 5 l2: 0.9244
Epoch: 5 l2_decoded: 0.8997
Training epoch 6 took 140.83 seconds. L2 loss: 0.9494
Training epoch 7 took 140.89 seconds. L2 loss: 0.9451
Training epoch 8 took 140.91 seconds. L2 loss: 0.9455
Training epoch 9 took 140.71 seconds. L2 loss: 0.9453
Training epoch 10 took 141.01 seconds. L2 loss: 0.9456
Epoch: 10 l2: 0.9239
Epoch: 10 l2_decoded: 0.9011
Training epoch 11 took 140.94 seconds. L2 loss: 0.9456
Training epoch 12 took 140.91 seconds. L2 loss: 0.9451
Training epoch 13 took 140.88 seconds. L2 loss: 0.9451
Training epoch 14 took 140.79 seconds. L2 loss: 0.9457
Training epoch 15 took 140.76 seconds. L2 loss: 0.9452
Epoch: 15 l2: 0.9270
Epoch: 15 l2_decoded: 0.9113
Training epoch 16 took 140.82 seconds. L2 loss: 0.9513
Training epoch 17 took 140.84 seconds. L2 loss: 0.9460
Training epoch 18 took 140.69 seconds. L2 loss: 0.9455
Training epoch 19 took 140.53 seconds. L2 loss: 0.9449
Training epoch 20 took 140.81 seconds. L2 loss: 0.9446
Epoch: 20 l2: 0.9255
Epoch: 20 l2_decoded: 0.9078
Training epoch 21 took 140.76 seconds. L2 loss: 0.9453
Training epoch 22 took 140.97 seconds. L2 loss: 0.9453
Training epoch 23 took 140.69 seconds. L2 loss: 0.9450
Training epoch 24 took 140.82 seconds. L2 loss: 0.9447
Training epoch 25 took 140.80 seconds. L2 loss: 0.9452
Epoch: 25 l2: 0.9240
Epoch: 25 l2_decoded: 0.9008
Training epoch 26 took 140.77 seconds. L2 loss: 0.9447
Training epoch 27 took 140.74 seconds. L2 loss: 0.9447
Training epoch 28 took 140.74 seconds. L2 loss: 0.9449
Training epoch 29 took 140.69 seconds. L2 loss: 0.9449
Training epoch 30 took 140.52 seconds. L2 loss: 0.9450
Epoch: 30 l2: 0.9244
Epoch: 30 l2_decoded: 0.9047
Training epoch 31 took 140.15 seconds. L2 loss: 0.9448
Training epoch 32 took 140.11 seconds. L2 loss: 0.9450
Training epoch 33 took 139.74 seconds. L2 loss: 0.9452
Training epoch 34 took 140.16 seconds. L2 loss: 0.9457
Training epoch 35 took 139.99 seconds. L2 loss: 0.9455
Epoch: 35 l2: 0.9239
Epoch: 35 l2_decoded: 0.9021
Training epoch 36 took 140.18 seconds. L2 loss: 0.9446
Training epoch 37 took 140.18 seconds. L2 loss: 0.9448
Training epoch 38 took 139.87 seconds. L2 loss: 0.9448
Training epoch 39 took 140.07 seconds. L2 loss: 0.9447
Training epoch 40 took 139.95 seconds. L2 loss: 0.9444
Epoch: 40 l2: 0.9249
Epoch: 40 l2_decoded: 0.9061
Training epoch 41 took 140.01 seconds. L2 loss: 0.9451
Training epoch 42 took 140.06 seconds. L2 loss: 0.9450
Training epoch 43 took 140.16 seconds. L2 loss: 0.9482
Training epoch 44 took 140.46 seconds. L2 loss: 0.9448
Training epoch 45 took 140.56 seconds. L2 loss: 0.9450
Epoch: 45 l2: 0.9243
Epoch: 45 l2_decoded: 0.9043
Training epoch 46 took 140.53 seconds. L2 loss: 0.9445
Training epoch 47 took 140.50 seconds. L2 loss: 0.9447
Training epoch 48 took 140.33 seconds. L2 loss: 0.9449
Training epoch 49 took 140.13 seconds. L2 loss: 0.9449
Training epoch 50 took 140.20 seconds. L2 loss: 0.9446
Epoch: 50 l2: 0.9243
Epoch: 50 l2_decoded: 0.9041
Training epoch 51 took 140.33 seconds. L2 loss: 0.9447
Training epoch 52 took 140.48 seconds. L2 loss: 0.9446
Training epoch 53 took 140.57 seconds. L2 loss: 0.9445
Training epoch 54 took 140.36 seconds. L2 loss: 0.9449
Training epoch 55 took 140.26 seconds. L2 loss: 0.9446
Epoch: 55 l2: 0.9243
Epoch: 55 l2_decoded: 0.9043
Training epoch 56 took 139.99 seconds. L2 loss: 0.9448
Training epoch 57 took 140.05 seconds. L2 loss: 0.9452
Training epoch 58 took 140.02 seconds. L2 loss: 0.9446
Training epoch 59 took 139.98 seconds. L2 loss: 0.9447
Training epoch 60 took 139.81 seconds. L2 loss: 0.9449
Epoch: 60 l2: 0.9240
Epoch: 60 l2_decoded: 0.9030
Training epoch 61 took 139.68 seconds. L2 loss: 0.9447
Training epoch 62 took 139.41 seconds. L2 loss: 0.9447
Training epoch 63 took 139.53 seconds. L2 loss: 0.9449
Training epoch 64 took 139.80 seconds. L2 loss: 0.9453
Training epoch 65 took 140.44 seconds. L2 loss: 0.9449
Epoch: 65 l2: 0.9245
Epoch: 65 l2_decoded: 0.9052
Training epoch 66 took 140.37 seconds. L2 loss: 0.9446
Training epoch 67 took 140.23 seconds. L2 loss: 0.9448
Training epoch 68 took 140.26 seconds. L2 loss: 0.9446
Training epoch 69 took 140.22 seconds. L2 loss: 0.9451
Training epoch 70 took 140.17 seconds. L2 loss: 0.9451
Epoch: 70 l2: 0.9246
Epoch: 70 l2_decoded: 0.9053
Training epoch 71 took 140.10 seconds. L2 loss: 0.9445
Training epoch 72 took 140.27 seconds. L2 loss: 0.9454
Training epoch 73 took 140.27 seconds. L2 loss: 0.9448
Training epoch 74 took 140.20 seconds. L2 loss: 0.9446
Training epoch 75 took 140.19 seconds. L2 loss: 0.9449
Epoch: 75 l2: 0.9254
Epoch: 75 l2_decoded: 0.9077
Training epoch 76 took 140.24 seconds. L2 loss: 0.9447
Training epoch 77 took 140.25 seconds. L2 loss: 0.9445
Training epoch 78 took 140.23 seconds. L2 loss: 0.9451
Training epoch 79 took 140.22 seconds. L2 loss: 0.9448
Training epoch 80 took 140.10 seconds. L2 loss: 0.9447
Epoch: 80 l2: 0.9241
Epoch: 80 l2_decoded: 0.9032
Training epoch 81 took 139.64 seconds. L2 loss: 0.9449
Training epoch 82 took 139.93 seconds. L2 loss: 0.9452
Training epoch 83 took 139.86 seconds. L2 loss: 0.9450
Training epoch 84 took 140.16 seconds. L2 loss: 0.9448
Training epoch 85 took 139.93 seconds. L2 loss: 0.9451
Epoch: 85 l2: 0.9242
Epoch: 85 l2_decoded: 0.9038
Training epoch 86 took 139.95 seconds. L2 loss: 0.9449
Training epoch 87 took 139.77 seconds. L2 loss: 0.9449
Training epoch 88 took 139.88 seconds. L2 loss: 0.9445
Training epoch 89 took 139.87 seconds. L2 loss: 0.9448
Training epoch 90 took 140.05 seconds. L2 loss: 0.9448
Epoch: 90 l2: 0.9246
Epoch: 90 l2_decoded: 0.9052
Training epoch 91 took 140.04 seconds. L2 loss: 0.9445
Training epoch 92 took 139.73 seconds. L2 loss: 0.9448
Training epoch 93 took 140.13 seconds. L2 loss: 0.9446
Training epoch 94 took 140.01 seconds. L2 loss: 0.9447
Training epoch 95 took 139.94 seconds. L2 loss: 0.9447
Epoch: 95 l2: 0.9244
Epoch: 95 l2_decoded: 0.9048
Training epoch 96 took 140.01 seconds. L2 loss: 0.9447
Training epoch 97 took 139.93 seconds. L2 loss: 0.9449
Training epoch 98 took 140.04 seconds. L2 loss: 0.9449
Training epoch 99 took 140.14 seconds. L2 loss: 0.9447
Epoch: 99 l2: 0.9246
Epoch: 99 l2_decoded: 0.9052
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:                    eval/l2 ‚ñÅ‚ñÑ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ
wandb:            eval/l2_decoded ‚ñÅ‚ñÉ‚ñÉ‚ñà‚ñÜ‚ñÉ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ
wandb:                 train/loss ‚ñá‚ñÜ‚ñà‚ñÖ‚ñÜ‚ñà‚ñà‚ñÖ‚ñÑ‚ñÇ‚ñÑ‚ñÉ‚ñá‚ñÇ‚ñÅ‚ñá‚ñÇ‚ñÜ‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñà‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñÖ‚ñá‚ñá‚ñà‚ñá‚ñà‚ñÉ‚ñÅ‚ñà‚ñÇ
wandb:                   train/lr ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ
wandb: train/train_epoch_duration ‚ñà‚ñÜ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÉ
wandb:             train/train_l2 ‚ñÑ‚ñÜ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÑ‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÇ‚ñÅ‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ
wandb: 
wandb: Run summary:
wandb:                    eval/l2 0.92456
wandb:            eval/l2_decoded 0.90521
wandb:                 train/loss 0.59359
wandb:                   train/lr 0.0005
wandb: train/train_epoch_duration 140.14043
wandb:             train/train_l2 0.94469
wandb: 
wandb: üöÄ View run FNOGNO-linear at: https://wandb.ai/nvr-ai-algo/ahmed-body/runs/638p52cm
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: logs/2023-07-03_20-27-04/wandb/run-20230703_202706-638p52cm/logs
